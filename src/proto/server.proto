syntax = "proto3";
package server;

import "common.proto";

// Inference
// Inference service exposed by the server in order to do distributed inference
service Inference {
    rpc do_inference(stream InferenceInput) returns (stream InferenceResponse) ;

    rpc assign_plan(AssignmentRequest) returns (AssignmentResponse);
}

message InferenceInput {
    common.RequestId request_id = 1;
    common.ComponentId component_id = 2;
    Tensor input_tensor = 3;
}

message Tensor {
    TensorInfo info = 1;
    TensorChunk tensor_chunk = 2;
}

message TensorInfo {
    string name = 1;
    string type = 2 ;
    repeated int32 shape = 3 ;
}

message TensorChunk {
    int32 chunk_size = 1;
    bytes chunk_data = 2;
}


message InferenceResponse {
    Tensor output_tensor = 1;
    optional float inference_time = 2 ;
}

message AssignmentRequest {
    string optimized_plan = 1;
}

message AssignmentResponse {}

// Execution Profile
// Service exposed in order to build execution profile for a certain model
service ExecutionProfile {
    rpc profile_execution(ExecutionProfileRequest) returns (ExecutionProfileResponse);
}

message ExecutionProfileRequest {
    common.ModelId model_id = 1 ;
}

message ExecutionProfileResponse {
    string profile = 1 ;
}